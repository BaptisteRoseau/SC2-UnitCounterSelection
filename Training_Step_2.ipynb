{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported packages.\n",
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.optimizers as opt\n",
    "import tensorflow.keras.layers as lr\n",
    "import tensorflow.keras.metrics as metrics\n",
    "from   tensorflow.keras.models import Sequential\n",
    "\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Imported packages.\")\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "\n",
    "## Loading the data generated through simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data.\n",
      "X_data shape: (10194, 33)\n"
     ]
    }
   ],
   "source": [
    "X_data_step1 = np.load(os.path.join(\"data\", \"MergedData\", \"terranBotInputData.npy\"))\n",
    "print(\"Loaded training data.\")\n",
    "print(\"X_data shape:\", X_data_step1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled data.\n"
     ]
    }
   ],
   "source": [
    "X_data_step1 = sk.utils.shuffle(X_data_step1, random_state=0)\n",
    "print(\"Shuffled data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing step1 model\n",
    "\n",
    "This model can predict the result of a fight. It is not perfect, but it is far faster than actually simulate the fight through the game.\n",
    "\n",
    "We will use it as our lost function for our reinforcment learning training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1_model(input_shape, output_shape):\n",
    "    model = Sequential([\n",
    "        lr.Dense(units=input_shape, activation='relu', input_dim=input_shape),\n",
    "        lr.Flatten(),\n",
    "        lr.Dropout(0.2),\n",
    "        lr.Dense(units=56, activation='relu'),\n",
    "        lr.Dropout(0.2),\n",
    "        lr.Dense(units=28, activation='relu'),\n",
    "        lr.Dropout(0.2),\n",
    "        lr.Dense(units=14, activation='relu'),\n",
    "        lr.Dropout(0.2),\n",
    "        lr.Dense(units=7, activation='relu'),\n",
    "        lr.Dense(units=output_shape, activation='tanh')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=opt.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0000001),\n",
    "              loss=\"mean_absolute_error\",\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1_model = step1_model(56, 1)\n",
    "step1_model.load_weights(os.path.join(\"data\", \"Models\", \"model_0.1343982719605969_step1.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining step2 model\n",
    "\n",
    "This model takes the units from the Terran, and returns the Zerg units requiered to beat them.\n",
    "\n",
    "That said, it will take an input array of shape 14 (33-5 for upgrades, divided by 2 as we don't need to give health information), and will return an output array of shape 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, output_shape):\n",
    "    model = Sequential([\n",
    "        lr.Dense(units=input_shape, activation='relu', input_dim=input_shape),\n",
    "        lr.Flatten(),\n",
    "        lr.Dropout(0.2),\n",
    "        lr.Dense(units=28, activation='relu'),\n",
    "        lr.Dropout(0.2),\n",
    "        lr.Dense(units=14, activation='relu'),\n",
    "        lr.Dropout(0.2),\n",
    "        lr.Dense(units=10, activation='relu'),\n",
    "        lr.Dense(units=output_shape, activation='tanh')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=opt.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0000001),\n",
    "              loss=\"mean_absolute_error\",\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1_model = create_model(14, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def format_array_to_step2(array):\n",
    "    croped = array[:-5:]\n",
    "    out = np.zeros((int(croped.shape[0]/2), ), dtype=np.float32)\n",
    "    for i in range(len(out)):\n",
    "        out[i] = croped[2*i]\n",
    "    return out\n",
    "\n",
    "@jit\n",
    "def format_array_to_step1(array):\n",
    "    out = np.zeros((array.shape[0]*2+5, ), dtype=np.float32)\n",
    "    for i in range(len(array)):\n",
    "        out[2*i] = array[i]\n",
    "        out[2*i +1 ] = 0. if array[i] == 0 else 1.\n",
    "    for i in range(5):\n",
    "        out[-i] = 1 # We assume that units are not upgraded\n",
    "    return out\n",
    "\n",
    "@jit\n",
    "def format_data_to_step2(data):\n",
    "    out = np.zeros((data.shape[0], int((data.shape[1]-5)/2)), dtype=np.float32)\n",
    "    for i in range(len(data)):\n",
    "       out[i] = format_array_to_step2(data[i])\n",
    "    return out\n",
    "\n",
    "@jit\n",
    "def format_data_to_step1(data):\n",
    "    out = np.zeros((data.shape[0], (data.shape[1]*2)+5), dtype=np.float32)\n",
    "    for i in range(len(data)):\n",
    "       out[i] = format_array_to_step1(data[i])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10194, 14)\n",
      "[ 1.  1. 22.  1.  0.  0.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  2.  2.  2.  2.  2.]\n",
      "[ 1. 22.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "(10194, 33)\n",
      "[ 1.  1. 22.  1.  0.  0.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  1.  1.  1.  1.]\n",
      "[ 1. 22.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Testing format_data_to_step2\n",
    "X_data_step2 = format_data_to_step2(X_data_step1)\n",
    "print(X_data_step2.shape)\n",
    "print(X_data_step1[0])\n",
    "print(X_data_step2[0])\n",
    "\n",
    "# Testing format_data_to_step1\n",
    "X_data_step1_bk = format_data_to_step1(X_data_step2)\n",
    "print(X_data_step1_bk.shape)\n",
    "print(X_data_step1_bk[0])\n",
    "print(X_data_step2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a simple model\n",
    "\n",
    "## Defining model \n",
    "\n",
    "The API used is tensorflow.keras as it is powerful and easy to use.\n",
    "\n",
    "The model itself only has Dense layers since there is no spacial information yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, output_shape):\n",
    "    model = Sequential([\n",
    "        lr.Dense(units=input_shape, activation='relu', input_dim=input_shape),\n",
    "        lr.Flatten(),\n",
    "        lr.Dropout(0.2),\n",
    "        lr.Dense(units=14, activation='relu'),\n",
    "        lr.Dropout(0.2),\n",
    "        lr.Dense(units=14, activation='relu'),\n",
    "        lr.Dropout(0.2),\n",
    "        lr.Dense(units=14, activation='relu'),\n",
    "        lr.Dropout(0.2),\n",
    "        lr.Dense(units=9, activation='relu'),\n",
    "        lr.Dense(units=output_shape, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=opt.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0000001),\n",
    "              loss=\"mean_absolute_error\",\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(X_data_step2[0].shape[0], 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Reinforcment Learning parameters\n",
    "\n",
    "Taken from one of keras-rl GitHub's page example : https://github.com/keras-rl/keras-rl/blob/master/examples/dqn_atari.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor, Env\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not done yet, but implement Processor and Environment following the link bellow\n",
    "#https://github.com/keras-rl/keras-rl/blob/216c3145f3dc4d17877be26ca2185ce7db462bad/rl/core.py#L515\n",
    "class CustomProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        return observation\n",
    "    \n",
    "    def process_state_batch(self, batch):\n",
    "        return batch\n",
    "    \n",
    "    def process_reward(self, reward):\n",
    "        return reward\n",
    "\n",
    "class CustomEnvironment(Env):\n",
    "    def step(self, action):\n",
    "        return action\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = CustomEnvironment()\n",
    "np.random.seed(123)\n",
    "env.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=4)\n",
    "processor = CustomProcessor()\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1.,\n",
    "                              value_min=.1, value_test=.05, nb_steps=1000000)\n",
    "nb_actions = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "len is not well defined for symbolic Tensors. (dense_16/Identity:0) Please call `x.shape` rather than `len(x)` for shape information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c004aa7051dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n\u001b[1;32m      2\u001b[0m                \u001b[0mprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps_warmup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                train_interval=4, delta_clip=1.)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.00025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, policy, test_policy, enable_double_dqn, enable_dueling_network, dueling_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Validate (important) input.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model \"{}\" has more than one output. DQN expects a model that has a single output.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m     raise TypeError(\"len is not well defined for symbolic Tensors. ({}) \"\n\u001b[1;32m    732\u001b[0m                     \u001b[0;34m\"Please call `x.shape` rather than `len(x)` for \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                     \"shape information.\".format(self.name))\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: len is not well defined for symbolic Tensors. (dense_16/Identity:0) Please call `x.shape` rather than `len(x)` for shape information."
     ]
    }
   ],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n",
    "               train_interval=4, delta_clip=1.)\n",
    "dqn.compile(Adam(lr=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "First, there is a check on the availability of a GPU. If you have an NVIDIA GPU on your computer, make sure CUDA 10.0, 10.1 and 10.2 are installed, are they are used by tensorlflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_STEPS = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Model on GPU if available\n",
    "list_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(list_devices))\n",
    "print(\"Using GPU: \", list_devices[0] if len(list_devices) > 0 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model_tilte_1\n",
    "device = tf.device('/GPU:0') if len(list_devices) > 0 else tf.device('/CPU:0')\n",
    "with tf.device('/GPU:0'):\n",
    "    history = dqn.fit(env, callbacks=callbacks, nb_steps=NB_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "This should be done directly through the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('data/Models/model_step2.h5')\n",
    "print(\"Saved model.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
